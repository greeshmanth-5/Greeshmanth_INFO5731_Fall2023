{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greeshmanth-5/Greeshmanth_INFO5731_Fall2023/blob/main/INFO5731_Assignment_Four_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biterm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43j9VOkXtyqZ",
        "outputId": "4be7ac61-99e5-46dd-be3e-72e390ae0952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biterm\n",
            "  Downloading biterm-0.1.5.tar.gz (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biterm) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from biterm) (4.66.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from biterm) (3.0.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from biterm) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->biterm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->biterm) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->biterm) (2023.6.3)\n",
            "Building wheels for collected packages: biterm\n",
            "  Building wheel for biterm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for biterm: filename=biterm-0.1.5-cp310-cp310-linux_x86_64.whl size=235694 sha256=6877de3da5c96fc00d002cd5cc6f6c6a7866848676b713fd7c1edb1a3a385356\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/6e/b8/b89e5a843baa692331b6c3db8d5f5ea43607ccad34426eebb4\n",
            "Successfully built biterm\n",
            "Installing collected packages: biterm\n",
            "Successfully installed biterm-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7ab639-b110-4052-981a-16f7c349e360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-06eb250506a1>:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  imdb_data['Cleaned Data'] = imdb_data['clean_text'].str.replace('[^\\w\\s]', '')  # Removal of punctuation\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100%|██████████| 10/10 [09:24<00:00, 56.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0 | Coherence=-19.01 | Top words= stephen film robbins best king dont hope great shawshank films\n",
            "Topic 1 | Coherence=26.34 | Top words= visuals process maintaining whisper redeem witness tends manages seekthis response\n",
            "Topic 2 | Coherence=-21.20 | Top words= freeman morgan robbins life hope great message acting prison performances\n",
            "Topic 3 | Coherence=-4.16 | Top words= solace award day tim bob state stephen inmate story time\n",
            "Topic 4 | Coherence=-20.63 | Top words= role office year classic box wonder prison comes nominations days\n",
            "Topic 5 | Coherence=-34.09 | Top words= movie film time hope times believe really redemption greatest final\n",
            "Topic 6 | Coherence=-19.44 | Top words= best great freeman story king robbins shawshank andy stephen acting\n",
            "Topic 7 | Coherence=-37.25 | Top words= movie dont dufresne performance modern years time picture different far\n",
            "Topic 8 | Coherence=-19.83 | Top words= movies make seven close day tough world think bob years\n",
            "Topic 9 | Coherence=-31.40 | Top words= time story like years close ive special robbins morgan young\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'coherence': [-19.009271313716898,\n",
              "  26.339592861277897,\n",
              "  -21.202301957140186,\n",
              "  -4.162693941030649,\n",
              "  -20.630099243732133,\n",
              "  -34.09481260902916,\n",
              "  -19.441245321980205,\n",
              "  -37.254279312038335,\n",
              "  -19.82711105633541,\n",
              "  -31.398986505827935],\n",
              " 'top_words': [array(['stephen', 'film', 'robbins', 'best', 'king', 'dont', 'hope',\n",
              "         'great', 'shawshank', 'films'], dtype=object),\n",
              "  array(['visuals', 'process', 'maintaining', 'whisper', 'redeem',\n",
              "         'witness', 'tends', 'manages', 'seekthis', 'response'],\n",
              "        dtype=object),\n",
              "  array(['freeman', 'morgan', 'robbins', 'life', 'hope', 'great', 'message',\n",
              "         'acting', 'prison', 'performances'], dtype=object),\n",
              "  array(['solace', 'award', 'day', 'tim', 'bob', 'state', 'stephen',\n",
              "         'inmate', 'story', 'time'], dtype=object),\n",
              "  array(['role', 'office', 'year', 'classic', 'box', 'wonder', 'prison',\n",
              "         'comes', 'nominations', 'days'], dtype=object),\n",
              "  array(['movie', 'film', 'time', 'hope', 'times', 'believe', 'really',\n",
              "         'redemption', 'greatest', 'final'], dtype=object),\n",
              "  array(['best', 'great', 'freeman', 'story', 'king', 'robbins',\n",
              "         'shawshank', 'andy', 'stephen', 'acting'], dtype=object),\n",
              "  array(['movie', 'dont', 'dufresne', 'performance', 'modern', 'years',\n",
              "         'time', 'picture', 'different', 'far'], dtype=object),\n",
              "  array(['movies', 'make', 'seven', 'close', 'day', 'tough', 'world',\n",
              "         'think', 'bob', 'years'], dtype=object),\n",
              "  array(['time', 'story', 'like', 'years', 'close', 'ive', 'special',\n",
              "         'robbins', 'morgan', 'young'], dtype=object)]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import LdaModel\n",
        "from biterm.utility import vec_to_biterms, topic_summuary\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from biterm.btm import oBTM\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "file_path = '/content/imdb_reviews_annotated_1.csv'\n",
        "imdb_data = pd.read_csv(file_path)\n",
        "\n",
        "# Cleaning the text data\n",
        "imdb_data['Cleaned Data'] = imdb_data['clean_text'].str.replace('[^\\w\\s]', '')  # Removal of punctuation\n",
        "imdb_data['Cleaned Data'] = imdb_data['Cleaned Data'].apply(lambda x: \" \".join(x.lower() for x in x.split()))  # Lowercase\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "imdb_data['Cleaned Data'] = imdb_data['Cleaned Data'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "# Tokenization and building the document-term matrix\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "words = [tokenizer.tokenize(x) for x in pd.Series(imdb_data['Cleaned Data'])]\n",
        "dictionary = corpora.Dictionary(words)\n",
        "corpus = [dictionary.doc2bow(word) for word in words]\n",
        "\n",
        "# LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20)\n",
        "lda_model.print_topics(num_topics=10, num_words=5)\n",
        "\n",
        "# Biterm Topic Model (oBTM)\n",
        "biterm = imdb_data['Cleaned Data'].values\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(biterm).toarray()  # Converting to tfidf matrix\n",
        "\n",
        "text = np.array(vectorizer.get_feature_names_out())  # Getting the feature names\n",
        "words = vec_to_biterms(X_tfidf)\n",
        "\n",
        "# Fit oBTM model\n",
        "btm_model = oBTM(num_topics=10, V=text)\n",
        "btm_model.fit_transform(words, iterations=10)\n",
        "\n",
        "# Getting the summary of topics\n",
        "topic_summuary(btm_model.phi_wz.T, X_tfidf, text, 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imdb_data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN7rce1nDreJ",
        "outputId": "a74af387-7d82-4db6-e8b4-ad546c409403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['document_id', 'clean_text', 'sentiment'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2537a109-5ac7-40a1-c6aa-45d862323b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in the test set:\n",
            "Positive    5\n",
            "Negative    2\n",
            "Name: sentiment, dtype: int64\n",
            "Accuracy of Multinomial Naive Bayes: 0.7142857142857143\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.00      0.00         2\n",
            "    Positive       0.71      1.00      0.83         5\n",
            "\n",
            "    accuracy                           0.71         7\n",
            "   macro avg       0.86      0.50      0.42         7\n",
            "weighted avg       0.80      0.71      0.60         7\n",
            "\n",
            "Cross-validation using MNB: 0.8\n",
            "Accuracy of SVM: 0.7142857142857143\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.00      0.00         2\n",
            "    Positive       0.71      1.00      0.83         5\n",
            "\n",
            "    accuracy                           0.71         7\n",
            "   macro avg       0.86      0.50      0.42         7\n",
            "weighted avg       0.80      0.71      0.60         7\n",
            "\n",
            "Cross-validation using SVM: 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-1fdeca43f0fb>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Cleaned Data'] = df['clean_text'].str.replace('[^\\w\\s]', '')  # Removal of punctuation\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\"\"\"I have added extra reviews which are negative and neutral in nature to perform this task as this\n",
        " task requires more than 1 value for the sentiment column and I have updated the csv file for this assignment.\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Replace the file path with the correct path to your CSV file\n",
        "file_path = '/content/imdb_reviews_annotated_1.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Cleaning the text data\n",
        "df['Cleaned Data'] = df['clean_text'].str.replace('[^\\w\\s]', '')  # Removal of punctuation\n",
        "df['Cleaned Data'] = df['Cleaned Data'].apply(lambda x: \" \".join(x.lower() for x in x.split()))  # Lowercase\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['Cleaned Data'] = df['Cleaned Data'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "# Replace 'sentiment' with the correct column name for sentiment\n",
        "sentiment_column = 'sentiment'\n",
        "\n",
        "# Splitting data into 80-20 train and test data\n",
        "tfidf_vector = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vector.fit_transform(df['Cleaned Data'])\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tfidf,\n",
        "                                                    df[sentiment_column],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Checking the distribution of classes in the test set\n",
        "print(\"Class distribution in the test set:\")\n",
        "print(y_test.value_counts())\n",
        "\n",
        "# Multinomial Naive Bayes model\n",
        "mnb = MultinomialNB()\n",
        "model_mnb = mnb.fit(x_train, y_train)\n",
        "y_pred_mnb = model_mnb.predict(x_test)\n",
        "print('Accuracy of Multinomial Naive Bayes:', accuracy_score(y_pred_mnb, y_test))\n",
        "print(classification_report(y_test, y_pred_mnb, zero_division=1))  # Setting zero_division=1 to handle zero divisions\n",
        "\n",
        "# Cross-validation using MNB\n",
        "scores_mnb = cross_val_score(mnb, x_test, y_test, cv=5)\n",
        "print(\"Cross-validation using MNB:\", scores_mnb.mean())\n",
        "\n",
        "# Linear Support Vector Machine (SVM) model\n",
        "svm = LinearSVC()\n",
        "model_svm = svm.fit(x_train, y_train)\n",
        "y_pred_svm = model_svm.predict(x_test)\n",
        "print('Accuracy of SVM:', accuracy_score(y_pred_svm, y_test))\n",
        "print(classification_report(y_test, y_pred_svm, zero_division=1))  # Setting zero_division=1 to handle zero divisions\n",
        "\n",
        "# Cross-validation using SVM\n",
        "# Use a smaller number of splits, e.g., cv=5\n",
        "scores_svm = cross_val_score(svm, x_test, y_test, cv=5)\n",
        "print(\"Cross-validation using SVM:\", scores_svm.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c61414-9aff-4a1c-f132-c2c97add44a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression R squared: 0.49830744666167914\n",
            "Root Mean Squared Error: 58860.9675541636\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the training and testing data\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Combine training and testing data for preprocessing\n",
        "combined_data = pd.concat([train_data.drop(\"SalePrice\", axis=1), test_data])\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "numeric_cols = combined_data.select_dtypes(include=np.number).columns\n",
        "categorical_cols = combined_data.select_dtypes(include='object').columns\n",
        "\n",
        "# Impute missing values for numeric columns\n",
        "numeric_imputer = SimpleImputer(strategy='mean')\n",
        "combined_data[numeric_cols] = numeric_imputer.fit_transform(combined_data[numeric_cols])\n",
        "\n",
        "# Impute missing values for categorical columns\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "combined_data[categorical_cols] = categorical_imputer.fit_transform(combined_data[categorical_cols])\n",
        "\n",
        "# Converting categorical to numerical using label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    combined_data[col] = label_encoder.fit_transform(combined_data[col])\n",
        "\n",
        "# Splitting back into training and testing data\n",
        "train_data_imputed = combined_data.iloc[:len(train_data)]\n",
        "test_data_imputed = combined_data.iloc[len(train_data):]\n",
        "\n",
        "# Scaling the data for better prediction\n",
        "scaler = MinMaxScaler()\n",
        "X = train_data_imputed\n",
        "y = train_data[\"SalePrice\"]\n",
        "\n",
        "X_scaled_min_max = scaler.fit_transform(X)\n",
        "X_scaled_min_max_df = pd.DataFrame(X_scaled_min_max, columns=X.columns)\n",
        "\n",
        "X_scaled_min_max_test = scaler.transform(test_data_imputed)\n",
        "X_scaled_min_max_df_test = pd.DataFrame(X_scaled_min_max_test, columns=test_data_imputed.columns)\n",
        "\n",
        "# Splitting data into train and test (80-20)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled_min_max_df, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Training the Linear Regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the model\n",
        "y_pred = regressor.predict(x_test)\n",
        "\n",
        "# Model evaluation\n",
        "print('Linear Regression R squared:', regressor.score(x_test, y_test))\n",
        "mse = mean_squared_error(y_pred, y_test)\n",
        "rmse = np.sqrt(mse)\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "\n",
        "# Predicting house prices for the test data\n",
        "predicted_prices = regressor.predict(X_scaled_min_max_df_test)\n",
        "\n",
        "# Displaying results\n",
        "test_data[\"Predicted_SalePrice\"] = predicted_prices\n",
        "test_data[['Id', 'Predicted_SalePrice']].to_csv('predicted_prices.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **pre-trained Large Language Model (LLM) from the Hugging Face Repository** for your specific task using the data collected in Assignment 3. After creating an account on Hugging Face (https://huggingface.co/), choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any Meta based text analysis model. Provide a brief description of the selected LLM, including its original sources, significant parameters, and any task-specific fine-tuning if applied.\n",
        "\n",
        "Perform a detailed analysis of the LLM's performance on your task, including key metrics, strengths, and limitations. Additionally, discuss any challenges encountered during the implementation and potential strategies for improvement. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the IMDb Reviews dataset\n",
        "df = pd.read_csv('/content/imdb_reviews_annotated - imdb_reviews_annotated (1).csv')\n",
        "\n",
        "# Map string labels to numerical labels\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x.lower() == 'positive' else 0)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define your dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        label = int(self.labels.iloc[idx])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,  # Explicitly set truncation to True\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Assuming 2 classes for sentiment\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = SentimentDataset(train_df['clean_text'], train_df['sentiment'], tokenizer, max_len=128)\n",
        "test_dataset = SentimentDataset(test_df['clean_text'], test_df['sentiment'], tokenizer, max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Increased batch size\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Increased batch size\n",
        "\n",
        "# Training parameters\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # Using torch.optim.AdamW\n",
        "epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['label']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['label']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        true_labels.extend(labels.numpy())\n",
        "        predicted_labels.extend(predictions.numpy())\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nkdiHFpos6y",
        "outputId": "5a7030dc-7a5f-45c4-b264-9dee8f6a414b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Print confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Print classification report\n",
        "class_report = classification_report(true_labels, predicted_labels)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kuh_fyr-r0hk",
        "outputId": "0a330374-9ac5-4135-81d3-431c4000238b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[2 0]\n",
            " [0 5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           1       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           1.00         7\n",
            "   macro avg       1.00      1.00      1.00         7\n",
            "weighted avg       1.00      1.00      1.00         7\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function to tokenize and predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        return_token_type_ids=False,\n",
        "        truncation=True,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        input_ids = encoding['input_ids']\n",
        "        attention_mask = encoding['attention_mask']\n",
        "        output = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(output.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "    return predicted_class, probs\n",
        "\n",
        "# Take input from the user\n",
        "user_input = input(\"Enter a movie review: \")\n",
        "\n",
        "# Predict sentiment\n",
        "predicted_class, class_probabilities = predict_sentiment(user_input)\n",
        "\n",
        "# Display results\n",
        "print(\"Predicted Class:\", predicted_class)\n",
        "print(\"Class Probabilities:\", class_probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMcGjdBVsTR3",
        "outputId": "234ee050-d916-4434-82e1-dd5be4e872f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a movie review: its an okay movie\n",
            "Predicted Class: 1\n",
            "Class Probabilities: tensor([[0.3912, 0.6088]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a dataset with columns like 'document_id', 'clean_text', and 'sentiment,' which involves sentiment analysis, we might consider using a pre-trained model specifically designed for natural language processing tasks. BERT (Bidirectional Encoder Representations from Transformers) and RoBERTa (Robustly optimized BERT approach) are popular choices for such tasks.\n",
        "\n",
        "I have considered BERT for this analysis:\n",
        "\n",
        "#### BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "**Description:**\n",
        "BERT, developed by Google, is a pre-trained transformer-based model for natural language processing tasks. Unlike traditional models that read text sequentially, BERT reads it bidirectionally, considering the entire context in a sentence. It has achieved state-of-the-art performance across various NLP tasks.\n",
        "\n",
        "**Original Sources:**\n",
        "BERT was introduced in the paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Devlin et al. The model is pretrained on a large corpus, including parts of BooksCorpus and English Wikipedia.\n",
        "\n",
        "**Significant Parameters:**\n",
        "- **Layers:** BERT consists of multiple layers (e.g., 12 layers in BERT base, 24 layers in BERT large).\n",
        "- **Attention Heads:** Each layer has multiple attention heads (e.g., 12 heads per layer in BERT base).\n",
        "- **Hidden Size:** The number of hidden units in each layer (e.g., 768 hidden units in BERT base).\n",
        "\n",
        "**Fine-Tuning:**\n",
        "For task-specific fine-tuning on sentiment analysis, you would typically add a classification layer on top of the BERT model and train it on your labeled sentiment data.\n",
        "\n",
        "### Analysis of BERT Performance:\n",
        "\n",
        "**Metrics:**\n",
        "- **Accuracy:** Overall correctness of sentiment predictions.\n",
        "- **Precision, Recall, F1 Score:** For each sentiment class, measuring the model's performance.\n",
        "- **Confusion Matrix:** Understanding where the model makes errors.\n",
        "\n",
        "**Strengths:**\n",
        "- **Context Understanding:** BERT captures bidirectional context, allowing it to understand the nuanced meaning of words.\n",
        "- **Transfer Learning:** Pre-trained on a massive dataset, BERT can be fine-tuned on smaller datasets for specific tasks.\n",
        "\n",
        "**Limitations:**\n",
        "- **Computational Intensity:** BERT is computationally expensive, both during training and inference.\n",
        "- **Data Requirements:** It requires substantial labeled data for fine-tuning.\n",
        "- **Token Limit:** BERT has a maximum token limit, and longer documents may need to be truncated.\n",
        "\n",
        "**Challenges:**\n",
        "- **Computational Resources:** Fine-tuning BERT may require significant computational resources.\n",
        "- **Data Annotation:** Obtaining a labeled dataset for sentiment analysis is crucial.\n",
        "\n",
        "**Strategies for Improvement:**\n",
        "- **Model Size:** Depending on your computational resources, you may choose a smaller variant of BERT (e.g., DistilBERT).\n",
        "- **Data Augmentation:** Augmenting your dataset with similar sentences to improve generalization.\n",
        "- **Ensemble Models:** Combining predictions from multiple models can enhance performance.\n",
        "\n",
        "Selecting the appropriate variant of BERT, managing computational resources, and addressing data-related challenges will be key to the successful implementation of sentiment analysis using a large language model like BERT."
      ],
      "metadata": {
        "id": "VG9CZt0cmohv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDEet1SvorkO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}