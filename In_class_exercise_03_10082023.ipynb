{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greeshmanth-5/Greeshmanth_INFO5731_Fall2023/blob/main/In_class_exercise_03_10082023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "### The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "source": [
        "## **Sentiment Analysis of Movie Reviews. In this task, we want to classify movie reviews as either \"positive\" or \"negative\" based on the sentiment expressed in the text.**\n",
        "\n",
        "### **1)Bag of Words (BoW) Features:**\n",
        "\n",
        "**Description :** The frequency of each word in the text is represented by BoW, which treats each word as a feature.\n",
        "\n",
        "**Use :** It detects the presence of specific words that indicate positive or negative emotion. Words such as \"excellent\" or \"awful\" can, for example, have a significant impact on sentiment.\n",
        "\n",
        "### **2)TF-IDF (Term Frequency-Inverse Document Frequency) Features:**\n",
        "\n",
        "**Description :** The TF-IDF algorithm computes the importance of each word in a document in relation to a corpus of documents.\n",
        "\n",
        "**Use :** It aids in identifying terms that are not only common in the document but also relatively uncommon in the total corpus. Rare words may include extra sentiment information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b8QGyEP1LWAc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmTpCxrVLK_z",
        "outputId": "2986f24b-4e40-4646-bdd1-77091bc6c421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW) Features:\n",
            "[[1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1]\n",
            " [0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0]]\n",
            "\n",
            "\n",
            "TF-IDF Features:\n",
            "[[0.40824829 0.         0.40824829 0.40824829 0.         0.\n",
            "  0.40824829 0.         0.         0.40824829 0.         0.40824829\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.4472136  0.4472136  0.         0.4472136 ]\n",
            " [0.         0.         0.         0.         0.40824829 0.40824829\n",
            "  0.         0.         0.40824829 0.         0.40824829 0.\n",
            "  0.40824829 0.         0.         0.40824829 0.        ]]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "sample_data = [\n",
        "    \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
        "    \"The acting was terrible, and the plot made no sense.\",\n",
        "    \"I'm not sure how to feel about this film. It had its moments, but overall, it was mediocre.\",\n",
        "]\n",
        "\n",
        "# Tokenization and Stopword Removal\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation and stopwords\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# Preprocess the sample data\n",
        "preprocessed_data = [preprocess_text(text) for text in sample_data]\n",
        "\n",
        "# Bag of Words (BoW) Features\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_features = count_vectorizer.fit_transform(preprocessed_data)\n",
        "\n",
        "print(\"Bag of Words (BoW) Features:\")\n",
        "print(bow_features.toarray())\n",
        "print(\"\\n\")\n",
        "\n",
        "# TF-IDF Features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_data)\n",
        "\n",
        "print(\"TF-IDF Features:\")\n",
        "print(tfidf_features.toarray())\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca7dd1f-8fa8-46d8-ed3f-bd88a4ec79ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking of BoW Features:\n",
            "Feature: terrible, Chi-Squared Score: 2.0000000000000004\n",
            "Feature: sense, Chi-Squared Score: 2.0000000000000004\n",
            "Feature: plot, Chi-Squared Score: 2.0000000000000004\n",
            "Feature: acting, Chi-Squared Score: 2.0000000000000004\n",
            "Feature: made, Chi-Squared Score: 2.0000000000000004\n",
            "Feature: loved, Chi-Squared Score: 0.5\n",
            "Feature: every, Chi-Squared Score: 0.5\n",
            "Feature: fantastic, Chi-Squared Score: 0.5\n",
            "Feature: feel, Chi-Squared Score: 0.5\n",
            "Feature: film, Chi-Squared Score: 0.5\n",
            "Feature: mediocre, Chi-Squared Score: 0.5\n",
            "Feature: sure, Chi-Squared Score: 0.5\n",
            "Feature: minute, Chi-Squared Score: 0.5\n",
            "Feature: moments, Chi-Squared Score: 0.5\n",
            "Feature: movie, Chi-Squared Score: 0.5\n",
            "Feature: overall, Chi-Squared Score: 0.5\n",
            "Feature: absolutely, Chi-Squared Score: 0.5\n",
            "\n",
            "\n",
            "Ranking of TF-IDF Features:\n",
            "Feature: terrible, Chi-Squared Score: 0.894427190999916\n",
            "Feature: sense, Chi-Squared Score: 0.894427190999916\n",
            "Feature: plot, Chi-Squared Score: 0.894427190999916\n",
            "Feature: acting, Chi-Squared Score: 0.894427190999916\n",
            "Feature: made, Chi-Squared Score: 0.894427190999916\n",
            "Feature: loved, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: every, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: fantastic, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: feel, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: film, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: mediocre, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: sure, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: minute, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: moments, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: movie, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: overall, Chi-Squared Score: 0.20412414523193156\n",
            "Feature: absolutely, Chi-Squared Score: 0.20412414523193156\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Sample text data\n",
        "sample_data = [\n",
        "    \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
        "    \"The acting was terrible, and the plot made no sense.\",\n",
        "    \"I'm not sure how to feel about this film. It had its moments, but overall, it was mediocre.\",\n",
        "]\n",
        "\n",
        "# Sample labels for sentiment (0: Negative, 1: Positive)\n",
        "labels = [1, 0, 1]\n",
        "\n",
        "# Tokenization and Stopword Removal\n",
        "# ... (same preprocessing steps as before)\n",
        "\n",
        "# Bag of Words (BoW) Features\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_features = count_vectorizer.fit_transform(preprocessed_data)\n",
        "\n",
        "# TF-IDF Features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_data)\n",
        "\n",
        "# Feature selection using chi-squared test\n",
        "def rank_features(features, labels):\n",
        "    chi2_scores, _ = chi2(features, labels)\n",
        "    feature_names = np.array(count_vectorizer.get_feature_names_out())\n",
        "    sorted_indices = np.argsort(chi2_scores)[::-1]\n",
        "\n",
        "    # Print feature names and their chi-squared scores in descending order\n",
        "    for i in sorted_indices:\n",
        "        print(f\"Feature: {feature_names[i]}, Chi-Squared Score: {chi2_scores[i]}\")\n",
        "\n",
        "# Rank features for BoW\n",
        "print(\"Ranking of BoW Features:\")\n",
        "rank_features(bow_features, labels)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Rank features for TF-IDF\n",
        "print(\"Ranking of TF-IDF Features:\")\n",
        "rank_features(tfidf_features, labels)\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KkBB9zLLK_1",
        "outputId": "e1f2e3c8-44dd-4031-c372-350feba0b40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSbzOVOiLK_1",
        "outputId": "30b68b0d-1f96-4a16-8ec4-5bc11beada94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3349979-43c6-4070-9b03-155d61e8f8f3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity Score: 0.8284611701965332\n",
            "Text: This movie is absolutely fantastic! I loved every minute of it.\n",
            "\n",
            "Similarity Score: 0.7313329577445984\n",
            "Text: I'm not sure how to feel about this film. It had its moments, but overall, it was mediocre.\n",
            "\n",
            "Similarity Score: 0.5957400798797607\n",
            "Text: The acting was terrible, and the plot made no sense.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data\n",
        "sample_data = [\n",
        "    \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
        "    \"The acting was terrible, and the plot made no sense.\",\n",
        "    \"I'm not sure how to feel about this film. It had its moments, but overall, it was mediocre.\",\n",
        "]\n",
        "\n",
        "# Define a query\n",
        "query = \"I enjoyed watching this movie. It was great!\"\n",
        "\n",
        "# Load BERT tokenizer and model (you can choose a different BERT variant if needed)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Encode the query and text data into BERT embeddings\n",
        "query_tokens = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "text_tokens = tokenizer(sample_data, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Get BERT embeddings for the query and text data\n",
        "with torch.no_grad():\n",
        "    query_embeddings = model(**query_tokens).last_hidden_state.mean(dim=1).numpy()\n",
        "    text_embeddings = model(**text_tokens).last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "cosine_similarities = cosine_similarity(query_embeddings, text_embeddings)\n",
        "\n",
        "# Rank the similarity scores in descending order\n",
        "sorted_indices = np.argsort(cosine_similarities[0])[::-1]\n",
        "\n",
        "# Print the ranked similarity scores and corresponding text data\n",
        "for index in sorted_indices:\n",
        "    print(f\"Similarity Score: {cosine_similarities[0][index]}\")\n",
        "    print(f\"Text: {sample_data[index]}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}