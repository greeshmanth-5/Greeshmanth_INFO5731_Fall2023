{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greeshmanth-5/Greeshmanth_INFO5731_Fall2023/blob/main/In_class_exercise_04_03282023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv3KRG_ejL8P"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-JDaWNPjL8S"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEtOYEjIjL8S"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "# Sample data\n",
        "sample_data = [\n",
        "    \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
        "    \"The acting was terrible, and the plot made no sense.\",\n",
        "    \"I'm not sure how to feel about this film. It had its moments, but overall, it was mediocre.\",\n",
        "]\n",
        "\n",
        "# Preprocess the sample data\n",
        "data_words = list(sent_to_words(sample_data))\n",
        "\n",
        "# Remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "# Number of topics\n",
        "num_topics = 2  # You can adjust the number of topics as needed\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                         id2word=id2word,\n",
        "                                         num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OOhVhEyKko7",
        "outputId": "f3ba6240-e82d-4c24-fcc4-140d7b2dcead"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.070*\"minute\" + 0.070*\"absolutely\" + 0.070*\"fantastic\" + 0.069*\"every\" + '\n",
            "  '0.068*\"movie\" + 0.066*\"loved\" + 0.057*\"moments\" + 0.057*\"film\" + '\n",
            "  '0.056*\"feel\" + 0.056*\"mediocre\"'),\n",
            " (1,\n",
            "  '0.069*\"sense\" + 0.066*\"made\" + 0.065*\"acting\" + 0.065*\"terrible\" + '\n",
            "  '0.064*\"plot\" + 0.062*\"overall\" + 0.062*\"sure\" + 0.061*\"mediocre\" + '\n",
            "  '0.061*\"feel\" + 0.061*\"film\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NucOeO7RjL8U"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, preprocess_string, strip_short, stem_text\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Sample data\n",
        "sample_data = [\n",
        "    \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
        "    \"The acting was terrible, and the plot made no sense.\",\n",
        "    \"I'm not sure how to feel about this film. It had its moments, but overall, it was mediocre.\",\n",
        "]\n",
        "\n",
        "# Create a DataFrame for the sample data\n",
        "df = pd.DataFrame({'Review': sample_data})\n",
        "\n",
        "# Preprocess the text using the same function\n",
        "def preprocess(text):\n",
        "    CUSTOM_FILTERS = [lambda x: x.lower(), remove_stopwords, strip_punctuation, strip_short, stem_text]\n",
        "    text = preprocess_string(text, CUSTOM_FILTERS)\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing to the sample data\n",
        "df['Review_Text (Clean)'] = df['Review'].apply(lambda x: preprocess(x))\n",
        "\n",
        "# Create a dictionary with the corpus\n",
        "corpus = df['Review_Text (Clean)']\n",
        "dictionary = corpora.Dictionary(corpus)\n",
        "\n",
        "# Convert corpus into a bag of words\n",
        "bow = [dictionary.doc2bow(text) for text in corpus]\n",
        "\n",
        "# Coherence score in topic modeling to measure how interpretable the topics are to humans.\n",
        "# Find the coherence score with a different number of topics\n",
        "for i in range(2, 11):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=df['Review_Text (Clean)'], dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))\n",
        "\n",
        "# Perform SVD on the bag of words with the LsiModel to extract 2 topics\n",
        "lsi = LsiModel(bow, num_topics=2, id2word=dictionary)\n",
        "\n",
        "# Find the 5 words with the strongest association to the derived topics\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))\n",
        "\n",
        "# Find the scores given between the review and each topic\n",
        "corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFamvLRwf3L5",
        "outputId": "923f45b3-2b1d-4fc5-872f-d33b3e1e88e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 2 clusters: 0.2974621508745988\n",
            "Coherence score with 3 clusters: 0.29746215087459865\n",
            "Coherence score with 4 clusters: 0.29746215087459865\n",
            "Coherence score with 5 clusters: 0.29746215087459876\n",
            "Coherence score with 6 clusters: 0.2974621508745989\n",
            "Coherence score with 7 clusters: 0.29746215087459865\n",
            "Coherence score with 8 clusters: 0.2974621508745987\n",
            "Coherence score with 9 clusters: 0.2974621508745988\n",
            "Coherence score with 10 clusters: 0.2974621508745987\n",
            "Words in 0: 0.408*\"film\" + 0.408*\"feel\" + 0.408*\"sure\" + 0.408*\"overal\" + 0.408*\"moment\" + 0.408*\"mediocr\" + 0.000*\"absolut\" + -0.000*\"fantast\" + -0.000*\"love\" + -0.000*\"minut\".\n",
            "Words in 1: 0.447*\"fantast\" + 0.447*\"absolut\" + 0.447*\"minut\" + 0.447*\"movi\" + 0.447*\"love\" + 0.000*\"feel\" + 0.000*\"sure\" + 0.000*\"film\" + 0.000*\"moment\" + 0.000*\"overal\".\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          [movi, absolut, fantast, love, minut]\n",
              "1                     [act, terribl, plot, sens]\n",
              "2    [sure, feel, film, moment, overal, mediocr]\n",
              "Name: Review_Text (Clean), dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWCTRiG0jL8U"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "!pip install preprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj0a57_kMsrk",
        "outputId": "baa353ab-3e8e-4b6a-e1b1-525e526324b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting preprocess\n",
            "  Downloading preprocess-2.0.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from preprocess) (0.18.3)\n",
            "Installing collected packages: preprocess\n",
            "Successfully installed preprocess-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCaEYdz6Msu2",
        "outputId": "266425da-2d4d-4733-e0b4-2fa73289a039"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.24.2 (from pyLDAvis)\n",
            "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.3)\n",
            "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
            "  Downloading pandas-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.8.7)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, tzdata, numpy, pandas, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.2 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 numpy-1.26.1 pandas-2.1.2 pyLDAvis-3.4.1 tzdata-2023.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data\n",
        "sample_data = [\n",
        "    \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
        "    \"The acting was terrible, and the plot made no sense.\",\n",
        "    \"I'm not sure how to feel about this film. It had its moments, but overall, it was mediocre.\",\n",
        "]\n",
        "\n",
        "# Preprocess the sample data\n",
        "sample_data_clean = [preprocess(text) for text in sample_data]\n",
        "\n",
        "# Join the preprocessed texts into a list of strings\n",
        "sample_data_clean_text = [' '.join(text) for text in sample_data_clean]\n",
        "\n",
        "# Create a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sample_data_clean_text)\n",
        "\n",
        "# Get the top words for each topic\n",
        "top = 10\n",
        "topic_to_topwords = {}\n",
        "for j in range(X.shape[0]):\n",
        "    top_words_indices = np.argsort(X[j].toarray()[0])[::-1][:top]\n",
        "    top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_indices]\n",
        "    msg = 'Topic %i has top words: %s' % (j, ', '.join(top_words))\n",
        "    print(msg)\n",
        "    topic_to_topwords[j] = top_words\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cugO8Q65Msxs",
        "outputId": "d006ea5a-ac7a-407d-ed08-a01686dc5f49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0 has top words: movi, minut, love, fantast, absolut, terribl, sure, sens, plot, overal\n",
            "Topic 1 has top words: terribl, sens, plot, act, sure, overal, movi, moment, minut, mediocr\n",
            "Topic 2 has top words: sure, overal, moment, mediocr, film, feel, terribl, sens, plot, movi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpWvCa93jL8V"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bertopic import BERTopic\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a function to clean text\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQIUaY2D5HH_",
        "outputId": "fab8cd97-4062-47cc-b693-e1c306ccc27c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
        "from bertopic import BERTopic\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data = fetch_20newsgroups(subset='all')['data']\n",
        "\n",
        "topic_model = BERTopic(nr_topics=\"auto\", calculate_probabilities=True, verbose=True)\n",
        "topics, _ = topic_model.fit_transform(data)\n",
        "\n",
        "topic_overview = topic_model.get_topic_freq()\n",
        "\n",
        "for topic_num, freq in topic_overview[1:].values:\n",
        "    topic_words = topic_model.get_topic(topic_num)\n",
        "    topic_summary = \", \".join([word[0] for word in topic_words[:5]])\n",
        "    print(f\"Topic {topic_num}: {topic_summary} (Freq: {freq})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "560a3273a5544c06bea32ecbd87dc503",
            "830fd5afb04b47bcac67a0378891e3fa",
            "cdc7e24c509a4bd3ae64ba58e14b8bd6",
            "153e628cc92b4e38905ddeeb347add21",
            "85c49e03fd414e0a9c4d596da5f8cbac",
            "c298add3f5264083b6baa389a49c4e23",
            "0844a195d89144ac8587f78211db55a0",
            "7ce2ffdaf74643c682b81114d3e6f0f0",
            "f1da8f6052ea4e8bb37f263b6d3a1e43",
            "c7e2bf5562bf493a83b9482ca2abd2a4",
            "5dd7f98a830f43b1991a2214594b4cc1"
          ]
        },
        "id": "ivwGZPiH9ChC",
        "outputId": "3567a33a-0577-424d-a735-12a4ad5ca8d4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "560a3273a5544c06bea32ecbd87dc503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/589 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-06 05:47:34,706 - BERTopic - Transformed documents to Embeddings\n",
            "2023-11-06 05:48:26,172 - BERTopic - Reduced dimensionality\n",
            "2023-11-06 05:51:00,415 - BERTopic - Clustered reduced embeddings\n",
            "2023-11-06 05:51:27,164 - BERTopic - Reduced number of topics from 379 to 247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: game, team, games, players, he (Freq: 1063)\n",
            "Topic 1: gun, the, president, that, to (Freq: 992)\n",
            "Topic 2: god, jesus, faith, atheists, that (Freq: 579)\n",
            "Topic 3: car, cars, ford, engine, mustang (Freq: 474)\n",
            "Topic 4: israel, israeli, jews, arab, jewish (Freq: 414)\n",
            "Topic 5: jpeg, image, gif, format, images (Freq: 300)\n",
            "Topic 6: armenian, turkish, armenians, were, armenia (Freq: 291)\n",
            "Topic 7: homosexual, homosexuality, gay, homosexuals, men (Freq: 220)\n",
            "Topic 8: mail, graphics, send, image, ray (Freq: 164)\n",
            "Topic 9: monitor, monitors, vga, nec, nanao (Freq: 155)\n",
            "Topic 10: printer, deskjet, hp, printers, laser (Freq: 148)\n",
            "Topic 11: modem, modems, serial, fax, courier (Freq: 147)\n",
            "Topic 12: amp, sale, sony, stereo, speakers (Freq: 146)\n",
            "Topic 13: colormap, color, bit, xv, cursor (Freq: 145)\n",
            "Topic 14: drivers, card, diamond, driver, ati (Freq: 124)\n",
            "Topic 15: cancer, lyme, patients, doctor, medical (Freq: 118)\n",
            "Topic 16: dos, windows, memory, allocation, emm386 (Freq: 111)\n",
            "Topic 17: microsoft, nt, os2, windows, challenge (Freq: 108)\n",
            "Topic 18: cop, traffic, behind, driving, bike (Freq: 108)\n",
            "Topic 19: sky, space, vandalizing, billboard, advertising (Freq: 106)\n",
            "Topic 20: sale, cds, cd, ticket, airline (Freq: 105)\n",
            "Topic 21: scsi, ide, scsi2, scsi1, drive (Freq: 104)\n",
            "Topic 22: drive, motherboard, sale, mhz, drives (Freq: 104)\n",
            "Topic 23: drive, drives, disk, controller, hard (Freq: 95)\n",
            "Topic 24: widget, motif, window, windows, server (Freq: 88)\n",
            "Topic 25: hell, eternal, god, heaven, jesus (Freq: 88)\n",
            "Topic 26: msg, food, sensitivity, chinese, superstition (Freq: 87)\n",
            "Topic 27: simms, simm, vram, 256k, quadra (Freq: 87)\n",
            "Topic 28: space, nasa, shuttle, launch, orbit (Freq: 84)\n",
            "Topic 29: window, expose, manager, events, event (Freq: 81)\n",
            "Topic 30: objective, morality, moral, values, odwyer (Freq: 80)\n",
            "Topic 31: moon, billion, prize, mining, lunar (Freq: 77)\n",
            "Topic 32: radar, detector, detectors, alarm, car (Freq: 76)\n",
            "Topic 33: error, libxmulibxmuso, symbol, compiling, compile (Freq: 73)\n",
            "Topic 34: morality, moral, murder, keith, society (Freq: 72)\n",
            "Topic 35: mormon, mormons, lds, church, bible (Freq: 68)\n",
            "Topic 36: insurance, health, private, care, canadian (Freq: 66)\n",
            "Topic 37: games, sega, genesis, sale, cd (Freq: 64)\n",
            "Topic 38: tax, taxes, income, vat, deficit (Freq: 60)\n",
            "Topic 39: mary, her, she, immaculate, conception (Freq: 59)\n",
            "Topic 40: government, libertarians, hendricks, stevehthoriscbrcom, libertarian (Freq: 57)\n",
            "Topic 41: copy, disks, protection, protected, program (Freq: 57)\n",
            "Topic 42: bmw, moa, joeridercactusorg, grips, bmws (Freq: 57)\n",
            "Topic 43: rushdie, islam, islamic, gregg, khomeini (Freq: 55)\n",
            "Topic 44: hst, mission, servicing, reboost, shuttle (Freq: 54)\n",
            "Topic 45: shift, shifting, manual, automatic, clutch (Freq: 52)\n",
            "Topic 46: dog, dogs, springer, bike, attack (Freq: 51)\n",
            "Topic 47: mouse, driver, jumpy, mousecom, motion (Freq: 50)\n",
            "Topic 48: gamma, bursters, oort, ray, cloud (Freq: 50)\n",
            "Topic 49: fonts, font, truetype, atm, tt (Freq: 50)\n",
            "Topic 50: 130, car, volvo, cars, speed (Freq: 49)\n",
            "Topic 51: des, s1, key, s2, keys (Freq: 48)\n",
            "Topic 52: battery, batteries, concrete, acid, lead (Freq: 48)\n",
            "Topic 53: candida, yeast, infections, systemic, bloom (Freq: 47)\n",
            "Topic 54: placebo, prozac, effects, depression, antihistamine (Freq: 45)\n",
            "Topic 55: love, petchgvg47gvgtekcom, god, daily, psalm (Freq: 44)\n",
            "Topic 56: helmet, helmets, shoei, jacket, fit (Freq: 43)\n",
            "Topic 57: des, key, scicrypt, cryptography, ciphers (Freq: 42)\n",
            "Topic 58: kuwait, saudi, iraq, war, gulf (Freq: 42)\n",
            "Topic 59: oil, drain, changing, plug, self (Freq: 42)\n",
            "Topic 60: command, spacecraft, galileo, pluto, timer (Freq: 42)\n",
            "Topic 61: countersteering, bike, countersteeringfaq, lean, technique (Freq: 41)\n",
            "Topic 62: station, space, redesign, shuttle, dcx (Freq: 41)\n",
            "Topic 63: cable, antenna, receiver, distance, measure (Freq: 41)\n",
            "Topic 64: cpu, fan, heat, sink, fans (Freq: 40)\n",
            "Topic 65: celp, speech, voice, compression, gtoalgtoalcom (Freq: 39)\n",
            "Topic 66: monitors, hours, 24, day, power (Freq: 38)\n",
            "Topic 67: photography, krillean, kirlian, pictures, leaf (Freq: 38)\n",
            "Topic 68: baptism, sin, aaron, infants, original (Freq: 38)\n",
            "Topic 69: mac, disks, 800k, read, 144mb (Freq: 37)\n",
            "Topic 70: skin, shots, allergy, fever, dry (Freq: 37)\n",
            "Topic 71: phone, line, number, onhook, tip (Freq: 37)\n",
            "Topic 72: list, ca, sale, guide, shipping (Freq: 36)\n",
            "Topic 73: polygon, polygons, routine, convex, fast (Freq: 36)\n",
            "Topic 74: science, methodology, scientific, homeopathy, sasghmtheseusunxsascom (Freq: 36)\n",
            "Topic 75: wave, bikers, waving, squids, cage (Freq: 36)\n",
            "Topic 76: images, image, ftp, otis, data (Freq: 35)\n",
            "Topic 77: drugs, drug, cocaine, illegal, marijuana (Freq: 35)\n",
            "Topic 78: irq, port, interrupt, com4, accessbus (Freq: 35)\n",
            "Topic 79: speed, 680x0, x86, 68040, processor (Freq: 35)\n",
            "Topic 80: tape, backup, adaptec, drive, 1542 (Freq: 35)\n",
            "Topic 81: monitor, screen, problem, video, 610 (Freq: 34)\n",
            "Topic 82: board, card, ethernet, connector, connectors (Freq: 33)\n",
            "Topic 83: sphere, points, den, radius, circle (Freq: 33)\n",
            "Topic 84: split, newsgroup, group, ch, aspects (Freq: 32)\n",
            "Topic 85: barbecued, carcinogenic, food, foods, taste (Freq: 32)\n",
            "Topic 86: king, black, kyle, adjective, cramm (Freq: 31)\n",
            "Topic 87: 610, centris, iivx, lciii, iici (Freq: 30)\n",
            "Topic 88: koresh, backing, stephen, biblical, b645zawutarlgutaedu (Freq: 30)\n",
            "Topic 89: answerfax, garrettingrescom, turkey, rickertnextworkrosehulmanedu, rickert (Freq: 30)\n",
            "Topic 90: tempest, monitor, emissions, holland, hollandcscolostateedu (Freq: 29)\n",
            "Topic 91: wax, scratches, plastic, paint, finish (Freq: 28)\n",
            "Topic 92: migraine, pain, migraines, banks, analgesics (Freq: 28)\n",
            "Topic 93: satan, ra, god, heaven, thou (Freq: 27)\n",
            "Topic 94: ftp, pctcp, renderman, nonibm, sites (Freq: 27)\n",
            "Topic 95: 1st, wolverine, comics, hulk, art (Freq: 27)\n",
            "Topic 96: centaur, proton, energy, uranium, protoncentaur (Freq: 27)\n",
            "Topic 97: neural, chemistry, molecular, 02106chopinudeledu, protein (Freq: 26)\n",
            "Topic 98: stove, electric, napalm, mfrheinwpiwpiedu, wood (Freq: 26)\n",
            "Topic 99: odometer, sensor, bmw, car, mileage (Freq: 25)\n",
            "Topic 100: summer, sublet, room, bedroom, dayton (Freq: 25)\n",
            "Topic 101: theory, evolution, universe, larson, larsons (Freq: 25)\n",
            "Topic 102: jesus, god, law, malcolm, punishable (Freq: 24)\n",
            "Topic 103: mithras, zoroastrians, oto, osiris, masonry (Freq: 24)\n",
            "Topic 104: slip, ashok, packet, winqvtnet, aiyar (Freq: 24)\n",
            "Topic 105: roby, robychopinudeledu, fbi, compound, fire (Freq: 24)\n",
            "Topic 106: marriage, married, ceremony, eyes, gods (Freq: 24)\n",
            "Topic 107: ir, detector, cycle, detection, pin (Freq: 24)\n",
            "Topic 108: freedom, beyer, speech, andi, aclu (Freq: 23)\n",
            "Topic 109: chromium, weight, fat, diet, wa7kgx (Freq: 23)\n",
            "Topic 110: solvent, adhesive, ducttape, mek, acetone (Freq: 23)\n",
            "Topic 111: ear, ears, hearing, earwax, dizziness (Freq: 23)\n",
            "Topic 112: ghostscript, postscript, pageview, ghostview, dsc (Freq: 23)\n",
            "Topic 113: abortion, abortions, choice, pay, health (Freq: 23)\n",
            "Topic 114: water, phd, dept, meteorologist, bacon (Freq: 22)\n",
            "Topic 115: koresh, batf, koreshs, he, children (Freq: 22)\n",
            "Topic 116: pregnency, teacher, biology, sperm, sex (Freq: 22)\n",
            "Topic 117: sound, midi, driver, blaster, soundblaster (Freq: 22)\n",
            "Topic 118: pillion, riding, advice, passenger, ride (Freq: 22)\n",
            "Topic 119: ham, surges, interference, alternator, power (Freq: 22)\n",
            "Topic 120: mhz, clock, operational, cpu, iisi (Freq: 22)\n",
            "Topic 121: lens, camera, nikon, goldbergoasysdtnavymil, zoom (Freq: 22)\n",
            "Topic 122: seizures, corn, seizure, cereals, foodrelated (Freq: 21)\n",
            "Topic 123: gas, tear, cs, riddle, j979jupitersuncsdunbca (Freq: 21)\n",
            "Topic 124: wheelie, wheelies, shaftdrive, xlyxvax5citcornelledu, shaftdrives (Freq: 21)\n",
            "Topic 125: maxaxaxaxaxaxaxaxaxaxaxaxaxaxax, mg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9vg9v, pwisemansalmonusdedu, cliff, maxaxaxaxaxaxaxaxaxaxaxaxaxaxaxq (Freq: 21)\n",
            "Topic 126: tickets, 105pm, 735pm, june, ticket (Freq: 21)\n",
            "Topic 127: orion, film, prototype, aviation, vacuum (Freq: 20)\n",
            "Topic 128: dialing, phones, tone, sweden, universal (Freq: 20)\n",
            "Topic 129: kidney, stones, she, calcium, b6 (Freq: 20)\n",
            "Topic 130: motif, linux, bindings, athena, 386bsd (Freq: 20)\n",
            "Topic 131: leds, blue, boards, green, solder (Freq: 20)\n",
            "Topic 132: r12, air, substitutes, conditioning, heat (Freq: 20)\n",
            "Topic 133: trinity, father, god, son, jesus (Freq: 20)\n",
            "Topic 134: lock, locks, cobra, kryptonite, cable (Freq: 20)\n",
            "Topic 135: sabbath, law, worship, ceremonial, jesus (Freq: 20)\n",
            "Topic 136: accelerations, acceleration, 45g, liquid, breathing (Freq: 20)\n",
            "Topic 137: rosicrucian, amorc, orders, tony, ch981clevelandfreenetedu (Freq: 20)\n",
            "Topic 138: scanner, scanners, logitech, scanman, price (Freq: 20)\n",
            "Topic 139: scope, scopes, dtmedincatbyteb30ingrcom, phosphor, digital (Freq: 19)\n",
            "Topic 140: joystick, joysticks, arcade, port, int15h (Freq: 19)\n",
            "Topic 141: geico, insurance, claim, companies, wonnacott (Freq: 19)\n",
            "Topic 142: chain, wax, maxima, spooge, cookson (Freq: 19)\n",
            "Topic 143: islamic, bcci, bank, gregg, jaegerbuphybuedu (Freq: 18)\n",
            "Topic 144: car, cr, safety, saftey, accident (Freq: 18)\n",
            "Topic 145: 42, tiff, philosophical, significance, joachimkihno (Freq: 18)\n",
            "Topic 146: tires, tire, fluids, brake, braking (Freq: 18)\n",
            "Topic 147: eye, dominance, prk, rk, righteye (Freq: 18)\n",
            "Topic 148: cancer, medical, circumcision, centers, center (Freq: 18)\n",
            "Topic 149: comet, jupiter, gehrels, orbit, temporary (Freq: 18)\n",
            "Topic 150: god, creates, omnipotence, us, godshaped (Freq: 18)\n",
            "Topic 151: adl, bullock, gerard, francisco, police (Freq: 18)\n",
            "Topic 152: duo, dock, apple, duodock, processor (Freq: 17)\n",
            "Topic 153: wire, wiring, ground, neutral, outlets (Freq: 17)\n",
            "Topic 154: w4wg, network, windows, workgroups, novell (Freq: 17)\n",
            "Topic 155: cd, cdrom, apple, cd300is, cd300 (Freq: 17)\n",
            "Topic 156: dx50, 50mhz, dx266, bus, 486dx (Freq: 17)\n",
            "Topic 157: powerbook, portable, pb100, pb, mac (Freq: 17)\n",
            "Topic 158: xv, escaped, copyright, donation, shareware (Freq: 17)\n",
            "Topic 159: weick, dana, him, cod, cpu (Freq: 17)\n",
            "Topic 160: gauge, gauges, cigarette, feagans, ashtrays (Freq: 17)\n",
            "Topic 161: church, churches, crossroads, movement, congregation (Freq: 17)\n",
            "Topic 162: engine, mr2, mr2s, noisy, eliot (Freq: 17)\n",
            "Topic 163: gun, buyback, guns, accidental, deaths (Freq: 17)\n",
            "Topic 164: janet, reno, children, madman, hostage (Freq: 17)\n",
            "Topic 165: numlock, emacs, xtermmap, keyboard, definekey (Freq: 17)\n",
            "Topic 166: cooling, towers, nuclear, plants, water (Freq: 17)\n",
            "Topic 167: diesels, diesel, fuel, emissions, injector (Freq: 16)\n",
            "Topic 168: uva, partying, schools, andi, beyer (Freq: 16)\n",
            "Topic 169: print, printer, file, postscript, manager (Freq: 16)\n",
            "Topic 170: letter, lobby, sammons, schoolrelated, letters (Freq: 16)\n",
            "Topic 171: prophecy, prophecies, lord, earthquake, prophesies (Freq: 16)\n",
            "Topic 172: mode, vga, vesa, 640x400, modes (Freq: 16)\n",
            "Topic 173: moment, silence, prayer, prayers, opposing (Freq: 16)\n",
            "Topic 174: eisa, isa, bus, board, vlb (Freq: 15)\n",
            "Topic 175: yassin, deir, irgun, dir, village (Freq: 15)\n",
            "Topic 176: icon, icons, program, manager, change (Freq: 15)\n",
            "Topic 177: drinking, riding, drink, drinks, sobriety (Freq: 15)\n",
            "Topic 178: sin, hate, love, sinner, our (Freq: 15)\n",
            "Topic 179: motorcycling, riders, declerck, declrckdrtsgmotcom, sportbike (Freq: 15)\n",
            "Topic 180: alzheimers, polio, disease, patients, postpolio (Freq: 15)\n",
            "Topic 181: software, level, process, wingert, bret (Freq: 15)\n",
            "Topic 182: cview, temp, directory, zenkar, mzmoscomcom (Freq: 15)\n",
            "Topic 183: movies, bikes, csundh30ursacalvinedu, sundheim, dod (Freq: 15)\n",
            "Topic 184: 8051, assemblers, microcontroller, oscar, mcolespock (Freq: 15)\n",
            "Topic 185: quadra, scsi, cartridge, mac, quadras (Freq: 15)\n",
            "Topic 186: mars, life, fred, spaceflight, space (Freq: 15)\n",
            "Topic 187: accessdigexnet, pat, express, online, communications (Freq: 15)\n",
            "Topic 188: needles, acupuncture, needle, aids, hypodermic (Freq: 15)\n",
            "Topic 189: uv, flashlight, bulbs, bulb, light (Freq: 14)\n",
            "Topic 190: cosmoproangmaralfalfacom, uunetbuedualphalphaproangmarcosmo, cosmoproangmar, proline, benson (Freq: 14)\n",
            "Topic 191: carbs, exhaust, intake, air, engine (Freq: 14)\n",
            "Topic 192: hacker, ethic, hackers, computer, dorsai (Freq: 14)\n",
            "Topic 193: cruel, keith, constitution, painful, iroquois (Freq: 14)\n",
            "Topic 194: logo, startup, wincom, rle, bs (Freq: 14)\n",
            "Topic 195: windy, wind, whenhow, yukyukyuk, helmetless (Freq: 14)\n",
            "Topic 196: font, fonts, dos, ssaunityncsuedu, alavi (Freq: 13)\n",
            "Topic 197: pope, schism, church, catholic, sspx (Freq: 13)\n",
            "Topic 198: varma, seema, ad, converter, pcboard (Freq: 13)\n",
            "Topic 199: het, de, een, van, vulcan (Freq: 13)\n",
            "Topic 200: updating, svein, winini, ini, utility (Freq: 13)\n",
            "Topic 201: wrench, srb, thiokol, tool, pliers (Freq: 13)\n",
            "Topic 202: temperature, sky, interstellar, radiation, dust (Freq: 13)\n",
            "Topic 203: fsk, mixer, fm, receiver, circuits (Freq: 13)\n",
            "Topic 204: god, love, shall, him, dpsnasakodakcom (Freq: 13)\n",
            "Topic 205: pds, slot, powerpc, lciii, socket (Freq: 13)\n",
            "Topic 206: duo, educational, price, eu, newsbytes (Freq: 13)\n",
            "Topic 207: octopus, ice, detroit, zazula, octopuses (Freq: 13)\n",
            "Topic 208: parsli, quisling, thomaspifiuiono, norway, thomas (Freq: 13)\n",
            "Topic 209: tongues, language, tounges, gifted, languages (Freq: 13)\n",
            "Topic 210: tank, tankbag, zipper, fj11001200, blaine (Freq: 13)\n",
            "Topic 211: vmax, handling, ba7116326ntuvaxntuacsg, handson, pls (Freq: 12)\n",
            "Topic 212: haldol, elderly, drugs, lithium, hospital (Freq: 12)\n",
            "Topic 213: selective, service, borden, abolish, reserve (Freq: 12)\n",
            "Topic 214: tape, copy, vcr, selfdestructing, tapes (Freq: 12)\n",
            "Topic 215: widgets, gadgets, widget, motif, dealy (Freq: 12)\n",
            "Topic 216: rocks, overpass, kids, erik, ejv2jvirginiaedu (Freq: 12)\n",
            "Topic 217: tga, rle, povray, pov, convert (Freq: 12)\n",
            "Topic 218: nubus, pds, lc, iii, slot (Freq: 12)\n",
            "Topic 219: date, clock, dos, bios, menu (Freq: 12)\n",
            "Topic 220: marriage, marry, eternal, ezekiel, parents (Freq: 12)\n",
            "Topic 221: deadly, lowabiding, rocks, kids, heracleous (Freq: 12)\n",
            "Topic 222: bryce, touring, overlook, bike, arches (Freq: 12)\n",
            "Topic 223: shaft, wheelies, chain, shaftdrives, efficient (Freq: 12)\n",
            "Topic 224: photography, disbeliever, zijdenbos, alexvusevanderbiltedu, paranormal (Freq: 12)\n",
            "Topic 225: beeps, chimes, error, ami, 607 (Freq: 12)\n",
            "Topic 226: easter, resurrection, celebration, pagan, goddess (Freq: 12)\n",
            "Topic 227: ampere, amp, db, company, ohmite (Freq: 12)\n",
            "Topic 228: sunset, sunrise, wetstein, times, jpwcbisecedrexeledu (Freq: 12)\n",
            "Topic 229: immunization, immunizations, care, health, kids (Freq: 11)\n",
            "Topic 230: iran, gulf, uae, iranian, irans (Freq: 11)\n",
            "Topic 231: habitable, planets, atmosphere, oxygen, planet (Freq: 11)\n",
            "Topic 232: opel, manta, buick, gt, gibbons (Freq: 11)\n",
            "Topic 233: sound, stereo, channel, mac, mono (Freq: 11)\n",
            "Topic 234: cache, iisi, powercache, fpu, card (Freq: 11)\n",
            "Topic 235: clutch, brake, honda, fluid, chatter (Freq: 11)\n",
            "Topic 236: uniforms, mets, marlins, uniform, reds (Freq: 11)\n",
            "Topic 237: duo, 230, apple, crashes, aftersleep (Freq: 11)\n",
            "Topic 238: dock, inflatable, envelope, space, boom (Freq: 11)\n",
            "Topic 239: eugenics, genes, memes, genome, we (Freq: 11)\n",
            "Topic 240: audio, relays, switching, clicks, lamp (Freq: 11)\n",
            "Topic 241: switch, beams, st11, bimmer, beamers (Freq: 11)\n",
            "Topic 242: workspace, managers, workspaces, workshift, bigdesk (Freq: 11)\n",
            "Topic 243: swap, mathcad, file, windows, disk (Freq: 10)\n",
            "Topic 244: vhs, kou, 1100, hiram, douglas (Freq: 10)\n",
            "Topic 245: media, news, cramer, ommitted, survey (Freq: 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he1Ev-jrjL8W"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->To compare topic modeling algorithms (LDA, NMF, LSA, and BERTopic),we have to consider\n",
        "factors such as topic quality and interpretability.\n",
        "\n",
        "-->LDA provides highly interpretable topics with word probability distributions.\n",
        "\n",
        "-->NMF and LSA offer reasonably interpretable topics but may differ from LDA.\n",
        "\n",
        "-->BERTopic can produce semantically meaningful topics but may differ from traditional methods due to its BERT model dependency.\n",
        "\n",
        "-->Evaluate topic coherence; LDA and NMF often have good coherence.\n",
        "\n",
        "-->LSA tends to have lower topic coherence.\n",
        "\n",
        "-->BERTopic can have good coherence, depending on the chosen BERT model.\n",
        "\n",
        "-->Consider scalability; LDA, NMF, and LSA are scalable to large datasets.\n",
        "\n",
        "-->BERTopic can be computationally intensive, especially with large BERT models.\n",
        "\n",
        "-->Assess hyperparameter sensitivity and model robustness for your project, and consider the availability of pre-trained models.\n",
        "\n",
        "-->Because of its explicit word probability distributions for each topic, LDA is frequently regarded as the best solution for interpretability.\n",
        "\n",
        "-->NMF and LSA can be good solutions if you want good interpretability with better scalability.\n",
        "\n",
        "-->BERTopic provides semantic context, but its interpretability varies and it may be more computationally intensive.\n",
        "\n",
        "Because of the aforementioned characteristics and by comparing results, I believe LSA is the most efficient of the four models.\n"
      ],
      "metadata": {
        "id": "6fNUh2T732ok"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "560a3273a5544c06bea32ecbd87dc503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_830fd5afb04b47bcac67a0378891e3fa",
              "IPY_MODEL_cdc7e24c509a4bd3ae64ba58e14b8bd6",
              "IPY_MODEL_153e628cc92b4e38905ddeeb347add21"
            ],
            "layout": "IPY_MODEL_85c49e03fd414e0a9c4d596da5f8cbac"
          }
        },
        "830fd5afb04b47bcac67a0378891e3fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c298add3f5264083b6baa389a49c4e23",
            "placeholder": "​",
            "style": "IPY_MODEL_0844a195d89144ac8587f78211db55a0",
            "value": "Batches: 100%"
          }
        },
        "cdc7e24c509a4bd3ae64ba58e14b8bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ce2ffdaf74643c682b81114d3e6f0f0",
            "max": 589,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1da8f6052ea4e8bb37f263b6d3a1e43",
            "value": 589
          }
        },
        "153e628cc92b4e38905ddeeb347add21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e2bf5562bf493a83b9482ca2abd2a4",
            "placeholder": "​",
            "style": "IPY_MODEL_5dd7f98a830f43b1991a2214594b4cc1",
            "value": " 589/589 [56:33&lt;00:00,  1.12s/it]"
          }
        },
        "85c49e03fd414e0a9c4d596da5f8cbac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c298add3f5264083b6baa389a49c4e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0844a195d89144ac8587f78211db55a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ce2ffdaf74643c682b81114d3e6f0f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1da8f6052ea4e8bb37f263b6d3a1e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7e2bf5562bf493a83b9482ca2abd2a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd7f98a830f43b1991a2214594b4cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}