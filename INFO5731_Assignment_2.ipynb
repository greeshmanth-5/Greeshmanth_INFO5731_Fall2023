{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greeshmanth-5/Greeshmanth_INFO5731_Fall2023/blob/main/INFO5731_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbCiwKypHG18",
        "outputId": "6a6f39a8-6abb-4296-bcb1-1eba5fe21418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to 'imdb_reviews.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define the URL of the IMDb film reviews page (replace with your desired film's URL).\n",
        "url = \"https://www.imdb.com/title/tt0111161/reviews\"\n",
        "\n",
        "# Send an HTTP GET request to the IMDb film reviews page.\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful.\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the page.\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Create a list to store the reviews data.\n",
        "    reviews_data = []\n",
        "\n",
        "    # Find the container that holds the user reviews (You may need to inspect the page source to find the correct element).\n",
        "    reviews_container = soup.find('div', class_='lister-list')\n",
        "\n",
        "    if reviews_container:\n",
        "        # Loop through the user reviews and extract relevant information.\n",
        "        for review in reviews_container.find_all('div', class_='text show-more__control'):\n",
        "            review_text = review.get_text(strip=True)\n",
        "            reviews_data.append({'Review Text': review_text})\n",
        "\n",
        "        # Save the data to a CSV file.\n",
        "        with open('imdb_reviews.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['Review Text']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(reviews_data)\n",
        "\n",
        "        print(f\"Data saved to 'imdb_reviews.csv'.\")\n",
        "    else:\n",
        "        print(\"Reviews container not found on the page.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to retrieve the IMDb film reviews page. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhZpLix_HG1-",
        "outputId": "6628156e-8884-4122-c975-747aefe5abc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-10-13 03:38:14.144572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-13 03:38:15.251106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhE4biuYHG1_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "# Load the IMDb reviews data from the CSV file\n",
        "df = pd.read_csv('imdb_reviews.csv')\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define functions for text cleaning and preprocessing\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def stem_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    stemmed_lemmatized_words = [token.lemma_ for token in doc]\n",
        "    return ' '.join(stemmed_lemmatized_words)\n",
        "\n",
        "# Apply the cleaning and preprocessing functions to the 'Review Text' column\n",
        "df['Cleaned Text'] = df['Review Text'].apply(clean_text)\n",
        "df['Cleaned Text'] = df['Cleaned Text'].apply(remove_stopwords)\n",
        "df['Cleaned Text'] = df['Cleaned Text'].apply(stem_and_lemmatize)\n",
        "\n",
        "# Save the cleaned data to the CSV file\n",
        "df.to_csv('imdb_reviews_cleaned.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f51934-cf83-4ed4-a396-f6215dc35599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Counts of POS Tags:\n",
            "PROPN: 703\n",
            "NOUN: 1670\n",
            "VERB: 575\n",
            "ADJ: 547\n",
            "ADV: 184\n",
            "AUX: 32\n",
            "SPACE: 131\n",
            "PART: 58\n",
            "SCONJ: 7\n",
            "INTJ: 12\n",
            "ADP: 32\n",
            "DET: 7\n",
            "NUM: 7\n",
            "PRON: 3\n",
            "X: 3\n",
            "SYM: 1\n",
            "\n",
            "Total Counts of Named Entities:\n",
            "darabont: 7\n",
            "rita hayworth: 4\n",
            "tim robbin morgan freeman: 2\n",
            "robbin banker: 1\n",
            "lover andy: 1\n",
            "redd freeman: 1\n",
            "andy prison: 1\n",
            "freeman: 9\n",
            "spring: 1\n",
            "norton: 1\n",
            "hadley william sadler: 1\n",
            "diamond s roger deakin lush: 1\n",
            "hank williams: 1\n",
            "seven: 4\n",
            "andy: 6\n",
            "ohio: 1\n",
            "tim robbin: 8\n",
            "andy dufresne: 4\n",
            "red redd play morgan freeman freeman: 1\n",
            "robbin: 1\n",
            "andy learn: 1\n",
            "brooks halten: 1\n",
            "second: 3\n",
            "oscar year: 1\n",
            "zero: 1\n",
            "american: 1\n",
            "firstly: 1\n",
            "today: 2\n",
            "dvd tim robbin: 1\n",
            "washington: 1\n",
            "andy robbin: 1\n",
            "zihuatanejo: 1\n",
            "andy pick: 1\n",
            "carefree andy: 1\n",
            "bob gunton: 3\n",
            "nixon: 1\n",
            "tom hank forr gump freeman: 1\n",
            "sean: 1\n",
            "mozart aria: 1\n",
            "recent year: 1\n",
            "year: 8\n",
            "freeman robbin: 1\n",
            "james whitmore: 3\n",
            "shine brilliantly: 1\n",
            "tim robbin send: 2\n",
            "appreciate year: 1\n",
            "freeman tim robbin: 1\n",
            "stephen king: 3\n",
            "imprison: 1\n",
            "tim robbins: 1\n",
            "morgan freeman: 5\n",
            "year year old: 1\n",
            "genre: 1\n",
            "william sadler: 1\n",
            "brian libby: 1\n",
            "guess m little late party: 1\n",
            "site year ago: 1\n",
            "tonight: 2\n",
            "brooks hatlens: 1\n",
            "mozart: 1\n",
            "mans: 1\n",
            "paul newman: 1\n",
            "couple decade: 1\n",
            "boyd redd morgan: 1\n",
            "andy person: 1\n",
            "andy come: 1\n",
            "summer: 1\n",
            "past year: 2\n",
            "scarytim robbin: 1\n",
            "little hour: 1\n",
            "half: 1\n",
            "luke: 1\n",
            "alcatraz: 1\n",
            "tim: 1\n",
            "luke bob stroud: 1\n",
            "robbin knowledge: 1\n",
            "dudley digge: 1\n",
            "cunne thomas newman: 1\n",
            "legendary hollywood: 1\n",
            "newman: 1\n",
            "andy red: 1\n",
            "brooks hatlen: 1\n",
            "brooks: 1\n",
            "robbin freeman: 1\n",
            "ellis boyd: 1\n",
            "red redd morgan freeman imprison year early red: 1\n",
            "andy experiences: 1\n",
            "beating guard: 1\n",
            "pasto colombiavia l: 1\n",
            "avid moviegoer: 1\n",
            "mexico central america usa: 1\n",
            "andy magician: 1\n",
            "english: 1\n",
            "morgan freeman ellis boyd: 1\n",
            "red redd: 1\n",
            "samuel norton: 1\n",
            "brooks andy: 1\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example sentence for explanation\n",
        "example_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Load the cleaned text from the CSV file\n",
        "df = pd.read_csv('imdb_reviews_cleaned.csv')\n",
        "\n",
        "# Initialize counters for POS tags and named entities\n",
        "pos_counter = Counter()\n",
        "entity_counter = Counter()\n",
        "\n",
        "# Process each cleaned text in the DataFrame\n",
        "for text in df['Cleaned Text']:\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # 1. Parts of Speech (POS) Tagging\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    pos_counter.update(pos_tags)\n",
        "\n",
        "    # 2. Constituency Parsing and Dependency Parsing\n",
        "    # Print one example sentence for explanation\n",
        "    if text == example_sentence:\n",
        "        print(\"Example Sentence:\", text)\n",
        "        # Constituency Parsing Tree\n",
        "        print(\"Constituency Parsing Tree:\")\n",
        "        for sent in doc.sents:\n",
        "            for token in sent:\n",
        "                print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}\")\n",
        "        # Dependency Parsing Tree\n",
        "        print(\"\\nDependency Parsing Tree:\")\n",
        "        for sent in doc.sents:\n",
        "            for token in sent:\n",
        "                print(f\"Token: {token.text}, Dependency Head: {token.head.text}, Dependency Relation: {token.dep_}\")\n",
        "\n",
        "    # 3. Named Entity Recognition (NER)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "    entity_counter.update(entities)\n",
        "\n",
        "# Display the total counts of POS tags and named entities\n",
        "print(\"Total Counts of POS Tags:\")\n",
        "for tag, count in pos_counter.items():\n",
        "    print(f\"{tag}: {count}\")\n",
        "\n",
        "print(\"\\nTotal Counts of Named Entities:\")\n",
        "for entity, count in entity_counter.items():\n",
        "    print(f\"{entity}: {count}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**\n",
        "\n",
        "***Constituency parsing :***\n",
        "Constituency parsing, also known as phrase structure parsing, is a natural language processing (NLP) technique used to analyse and describe the grammatical structure of a sentence. The fundamental purpose of constituency parsing is to identify the constituents or subparts of a sentence and expose how words are grouped together to form bigger units, such as noun phrases (NP), verb phrases (VP), prepositional phrases (PP), and more. This parsing technique divides a text into hierarchical structures, demonstrating how distinct words and phrases are organised and connected inside the sentence.\n",
        "\n",
        "Key characteristics of constituency parsing include:\n",
        "\n",
        "\n",
        "1.   Hierarchical Structure\n",
        "2.   Phrases and Constituents\n",
        "\n",
        "3.  Context-Free Grammar\n",
        "2.  Parse Trees\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "***Dependency parsing :***\n",
        "Dependency parsing is a natural language processing (NLP) technique for analysing the grammatical structure of a phrase by studying the links between words and their dependencies. Dependency parsing, as opposed to constituency parsing, determines how words in a sentence depend on or relate to one another. It describes the syntactic structure of a sentence in terms of directed dependencies or grammatical relationships between words.\n",
        "\n",
        "Key characteristics of dependency parsing include:\n",
        "\n",
        "\n",
        "1.   Directed Dependencies\n",
        "\n",
        "1.   Root Node\n",
        "\n",
        "1.   Types of Dependencies\n",
        "2.   No Phrases or Constituents\n",
        "\n",
        "1.   Simplicity and Elegance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}