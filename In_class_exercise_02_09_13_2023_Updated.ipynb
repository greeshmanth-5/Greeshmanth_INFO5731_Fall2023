{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greeshmanth-5/Greeshmanth_INFO5731_Fall2023/blob/main/In_class_exercise_02_09_13_2023_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ],
      "metadata": {
        "id": "mAzh1U0sE5I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ],
      "metadata": {
        "id": "PpgvZQdRE-HV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Question:**\n",
        "\n",
        "The research question I have chosen revolves around the world's top-rated books over time. For this endeavor, we can harness data from diverse platforms such as Goodreads, Amazon, Barnes & Noble, etc. However, I've decided to concentrate on Goodreads for this exercise, given its robust user base and wide variety of reviews.\n",
        "\n",
        "I aim to amass data on the top 1500 books of all time and intend to focus on fields like book title, publication year, average rating, number of ratings, and author. The data on Goodreads, being inherently unstructured, poses a challenge. Therefore, BeautifulSoup comes in handy to traverse through the website's structure, enabling us to mine structured data from its raw form.\n",
        "\n"
      ],
      "metadata": {
        "id": "blnHOrBjQ_IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "00jrL1j1Q4jM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QpWOgjHi9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687ab130-5239-4f43-d61c-b01c90cbfcf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Title               Author  \\\n",
            "0    Harry Potter and the Order of the Phoenix (Har...         J.K. Rowling   \n",
            "1                                  Pride and Prejudice          Jane Austen   \n",
            "2                                To Kill a Mockingbird           Harper Lee   \n",
            "3                                       The Book Thief         Markus Zusak   \n",
            "4                     Twilight (The Twilight Saga, #1)      Stephenie Meyer   \n",
            "..                                                 ...                  ...   \n",
            "984                                        The Tempest  William Shakespeare   \n",
            "985  Finding Hope in the Darkness of Grief: Spiritu...    Diamante Lavendar   \n",
            "986                                 Along for the Ride         Sarah Dessen   \n",
            "987                             Smilla's Sense of Snow           Peter Høeg   \n",
            "988                      Blue Bloods (Blue Bloods, #1)   Melissa de la Cruz   \n",
            "\n",
            "                                  Rating  \n",
            "0    4.50 avg rating — 3,227,132 ratings  \n",
            "1    4.28 avg rating — 4,050,671 ratings  \n",
            "2    4.26 avg rating — 5,823,012 ratings  \n",
            "3    4.39 avg rating — 2,429,010 ratings  \n",
            "4    3.64 avg rating — 6,279,938 ratings  \n",
            "..                                   ...  \n",
            "984    3.79 avg rating — 203,428 ratings  \n",
            "985      4.29 avg rating — 1,562 ratings  \n",
            "986    4.04 avg rating — 205,056 ratings  \n",
            "987     3.74 avg rating — 48,162 ratings  \n",
            "988    3.70 avg rating — 140,143 ratings  \n",
            "\n",
            "[989 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_books_from_goodreads(page_num):\n",
        "    URL = f\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page={page_num}\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
        "\n",
        "    response = requests.get(URL, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    table = soup.find(\"table\", class_=\"tableList js-dataTooltip\")\n",
        "    rows = table.find_all(\"tr\")\n",
        "\n",
        "    books_data = []\n",
        "\n",
        "    for row in rows[1:]:\n",
        "        title_elem = row.find(\"a\", class_=\"bookTitle\")\n",
        "        title = title_elem.text.strip()\n",
        "\n",
        "        author_elem = row.find(\"a\", class_=\"authorName\")\n",
        "        author = author_elem.text.strip()\n",
        "\n",
        "        rating_elem = row.find(\"span\", class_=\"minirating\")\n",
        "        rating = rating_elem.text.strip()\n",
        "\n",
        "        books_data.append({\"Title\": title, \"Author\": author, \"Rating\": rating})\n",
        "\n",
        "    return books_data\n",
        "\n",
        "def main():\n",
        "    all_books_data = []\n",
        "\n",
        "    # Adjust the range to determine how many pages to scrape (each page has 100 books)\n",
        "    for i in range(1, 11): # Scraping 10 pages as an example\n",
        "\n",
        "        books = get_books_from_goodreads(i)\n",
        "        all_books_data.extend(books)\n",
        "\n",
        "    df = pd.DataFrame(all_books_data)\n",
        "    print(df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "class SemanticScholarAPI:\n",
        "\n",
        "    BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def fetch_data(self, topic, offset=0, limit=10):\n",
        "        params = {\n",
        "            \"query\": topic,\n",
        "            \"offset\": offset,\n",
        "            \"limit\": limit,\n",
        "            \"fields\": \"url,title,abstract,authors\"\n",
        "        }\n",
        "        response = self.session.get(self.BASE_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        return response.json()['data']\n",
        "\n",
        "def main():\n",
        "    api = SemanticScholarAPI()\n",
        "    topic = 'Money'\n",
        "\n",
        "    try:\n",
        "        data = api.fetch_data(topic, offset=100, limit=3)\n",
        "        df = pd.DataFrame(data)\n",
        "        print(df.head(1000))\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching data for topic {topic}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-PYXE8QJgy2",
        "outputId": "f2e09b83-39ba-49e0-cb88-83bf4100b6fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    paperId  \\\n",
            "0  ff279098481a87faa4498fa9088cd9bd835e9a3e   \n",
            "1  b2c827333b780d4f76b56c6e90bb9141dc31278f   \n",
            "2  ac0182df6bbf38fb68911309ff729d9edb24c8b0   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.semanticscholar.org/paper/ff279098...   \n",
            "1  https://www.semanticscholar.org/paper/b2c82733...   \n",
            "2  https://www.semanticscholar.org/paper/ac0182df...   \n",
            "\n",
            "                                               title  \\\n",
            "0  More than fun and money. Worker Motivation in ...   \n",
            "1  Money and Thinking: Reminders of Money Trigger...   \n",
            "2  ‘Bridges to cash’ : Channelling agency in mobi...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  The payment in paid crowdsourcing markets like...   \n",
            "1  The idea of money reminds consumers of persona...   \n",
            "2  Encountered by mobile money professionals - in...   \n",
            "\n",
            "                                             authors  \n",
            "0  [{'authorId': '2563486', 'name': 'Nicolas Kauf...  \n",
            "1  [{'authorId': '49419254', 'name': 'Jochim Hans...  \n",
            "2  [{'authorId': '40210117', 'name': 'B. Maurer'}...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do either of the question-4 tasks given below."
      ],
      "metadata": {
        "id": "yCQpbJnwTxAB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FymVNKVi9-pb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 (10 points):\n",
        "\n",
        "In this task, you are required to identify and utilize online tools for web scraping data from websites without the need for coding, with a specific focus on Parsehub. The objective is to gather data and save it in formats like CSV, Excel, or any other suitable file format.\n",
        "\n",
        "You have to mention an introduction to the tool which ever you prefer to use, steps to follow for web scrapping and the final output of the data collected.\n",
        "\n",
        "Upload a document (Word or PDF File) in the same repository and you can add the link in the ipynb file."
      ],
      "metadata": {
        "id": "wOeAr9TJTBgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the link to the document here\n",
        "https://github.com/greeshmanth-5/Greeshmanth_INFO5731_Fall2023/blob/main/Introduction%20to%20ParseHub.docx.pdf"
      ],
      "metadata": {
        "id": "N20TjXLmTG1u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}